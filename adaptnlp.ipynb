{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "pandas.core.frame.DataFrame"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loader(file, is_number = False):\n",
    "    data = []\n",
    "    with open(file, encoding=\"utf8\") as my_file:\n",
    "        data = my_file.read().splitlines()\n",
    "    if is_number:\n",
    "        data = [int(i) for i in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data = loader(\"Raw/test_text.txt\")\n",
    "test_labels = loader(\"Raw/test_labels.txt\", True)\n",
    "val_data = loader(\"Raw/val_text.txt\")\n",
    "val_labels = loader(\"Raw/val_labels.txt\", True)\n",
    "train_data = loader(\"Raw/train_text.txt\")\n",
    "train_labels = loader(\"Raw/train_labels.txt\", True)\n",
    "mappings = {0:\"anger\", 1:\"joy\", 2:\"optimism\",3:\t\"sadness\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame({\"text\":test_data, \"label\":test_labels, \"emotion\":[mappings[i] for i in test_labels]})\n",
    "val = pd.DataFrame({\"text\":val_data, \"label\":val_labels, \"emotion\":[mappings[i] for i in val_labels]})\n",
    "train = pd.DataFrame({\"text\":train_data, \"label\":train_labels, \"emotion\":[mappings[i] for i in train_labels]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = train[train.label!=3]\n",
    "val = val[val.label!=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                                                                                           text  \\\n0                                     “Worry is a down payment on a problem you may never have'.  Joyce Meyer.  #motivation #leadership #worry    \n1                                           My roommate: it's okay that we can't spell because we have autocorrect. #terrible #firstworldprobs    \n2                                               No but that's so cute. Atsu was probably shy about photos before but cherry helped her out uwu    \n3                                       Rooneys fucking untouchable isn't he? Been fucking dreadful again, depay has looked decent(ish)tonight    \n4                                                                            it's pretty depressing when u hit pan on ur favourite highlighter    \n...                                                                                                                                         ...   \n3252  I get discouraged because I try for 5 fucking years a contact with Lady Gaga but are thousands of tweets, how she would see my tweet? :(    \n3253                                                             The @user are in contention and hosting @user nation and Camden is empty #sad    \n3254                                           @user @user @user @user @user as a fellow UP grad, i shiver at the shallowness of his arguments    \n3255                                                                   You have a #problem? Yes! Can you do #something about it? No! Than why     \n3256         @user @user i will fight this guy! Don't insult the lions like that! But seriously they kinda are.Wasted some of the best players    \n\n      label   emotion  \n0         2  optimism  \n1         0     anger  \n2         1       joy  \n3         0     anger  \n4         3   sadness  \n...     ...       ...  \n3252      3   sadness  \n3253      3   sadness  \n3254      0     anger  \n3255      0     anger  \n3256      0     anger  \n\n[3257 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>“Worry is a down payment on a problem you may never have'.  Joyce Meyer.  #motivation #leadership #worry</td>\n      <td>2</td>\n      <td>optimism</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>My roommate: it's okay that we can't spell because we have autocorrect. #terrible #firstworldprobs</td>\n      <td>0</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>No but that's so cute. Atsu was probably shy about photos before but cherry helped her out uwu</td>\n      <td>1</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Rooneys fucking untouchable isn't he? Been fucking dreadful again, depay has looked decent(ish)tonight</td>\n      <td>0</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>it's pretty depressing when u hit pan on ur favourite highlighter</td>\n      <td>3</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3252</th>\n      <td>I get discouraged because I try for 5 fucking years a contact with Lady Gaga but are thousands of tweets, how she would see my tweet? :(</td>\n      <td>3</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>3253</th>\n      <td>The @user are in contention and hosting @user nation and Camden is empty #sad</td>\n      <td>3</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>3254</th>\n      <td>@user @user @user @user @user as a fellow UP grad, i shiver at the shallowness of his arguments</td>\n      <td>0</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>3255</th>\n      <td>You have a #problem? Yes! Can you do #something about it? No! Than why</td>\n      <td>0</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>3256</th>\n      <td>@user @user i will fight this guy! Don't insult the lions like that! But seriously they kinda are.Wasted some of the best players</td>\n      <td>0</td>\n      <td>anger</td>\n    </tr>\n  </tbody>\n</table>\n<p>3257 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import adaptnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from adaptnlp  import HFModelHub, HF_TASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hub = HFModelHub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models = hub.search_model_by_task(HF_TASKS.TEXT_CLASSIFICATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Model Name: distilbert-base-uncased-finetuned-sst-2-english, Tasks: [text-classification],\n Model Name: roberta-large-mnli, Tasks: [text-classification],\n Model Name: roberta-base-openai-detector, Tasks: [text-classification],\n Model Name: roberta-large-openai-detector, Tasks: [text-classification]]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = models[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Model Name: roberta-large-mnli, Tasks: [text-classification]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from adaptnlp import TaskDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from adaptnlp import TaskDatasets,SequenceClassificationDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_y(items:pd.DataFrame):\n",
    "    idxs = (items[\"is_valid\"].values.astype('bool'))\n",
    "    train_idxs, valid_idxs = [], []\n",
    "    for i,idx in enumerate(idxs):\n",
    "        train_idxs.append(i) if idx else valid_idxs.append(i)\n",
    "    return (train_idxs, valid_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c74766c42049414ba942e38753e2b75e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cd0c0fb63d048ff9dee7c44824e3332"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dsets = SequenceClassificationDatasets.from_dfs(\n",
    "    train,\n",
    "    text_col='text',\n",
    "    label_col='label',\n",
    "    tokenizer_name=model.name,\n",
    "    tokenize=True,\n",
    "    valid_df=val,\n",
    "    tokenize_kwargs={'max_length':68, 'truncation':True, 'padding':True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dls = dsets.dataloaders(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from adaptnlp import SequenceClassificationTuner\n",
    "\n",
    "tuner = SequenceClassificationTuner(dls, model.name, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "f1_score\n"
     ]
    }
   ],
   "source": [
    "_ = [print(m.name) for m in tuner.metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "FlattenedLoss of CrossEntropyLoss()"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.0003981071640737355)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    \n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:79] data. DefaultCPUAllocator: not enough memory: you tried to allocate 17825792 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8332/1162452228.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0madaptnlp\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mStrategy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mtuner\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstrategy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mStrategy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOneCycle\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\adaptnlp\\training\\core.py\u001B[0m in \u001B[0;36mtune\u001B[1;34m(self, epochs, lr, strategy, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    411\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mattr\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;34m'epochs,lr,cbs'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    412\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mattr\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 413\u001B[1;33m         \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcbs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    414\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    415\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mdelegates\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mLearner\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlr_find\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\fastai\\callback\\schedule.py\u001B[0m in \u001B[0;36mfit_one_cycle\u001B[1;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt)\u001B[0m\n\u001B[0;32m    114\u001B[0m     scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n\u001B[0;32m    115\u001B[0m               'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n\u001B[1;32m--> 116\u001B[1;33m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcbs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mParamScheduler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mscheds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mL\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcbs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreset_opt\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreset_opt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwd\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mwd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    117\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[1;31m# Cell\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\fastai\\learner.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001B[0m\n\u001B[0;32m    219\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_hypers\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlr\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mlr\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mn_epoch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mn_epoch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 221\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_with_events\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_fit\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'fit'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mCancelFitException\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_end_cleanup\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    222\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_end_cleanup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdl\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mxb\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0myb\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpred\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\fastai\\learner.py\u001B[0m in \u001B[0;36m_with_events\u001B[1;34m(self, f, event_type, ex, final)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    162\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_with_events\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfinal\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnoop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 163\u001B[1;33m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'before_{event_type}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m;\u001B[0m  \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    164\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mex\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'after_cancel_{event_type}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    165\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'after_{event_type}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m;\u001B[0m  \u001B[0mfinal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\fastai\\learner.py\u001B[0m in \u001B[0;36m_do_fit\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    210\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mn_epoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    211\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 212\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_with_events\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'epoch'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mCancelEpochException\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    213\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    214\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwd\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcbs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreset_opt\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\fastai\\learner.py\u001B[0m in \u001B[0;36m_with_events\u001B[1;34m(self, f, event_type, ex, final)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    162\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_with_events\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfinal\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnoop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 163\u001B[1;33m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'before_{event_type}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m;\u001B[0m  \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    164\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mex\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'after_cancel_{event_type}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    165\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'after_{event_type}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m;\u001B[0m  \u001B[0mfinal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\fastai\\learner.py\u001B[0m in \u001B[0;36m_do_epoch\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    205\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_do_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 206\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_epoch_train\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    207\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_epoch_validate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    208\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\fastai\\learner.py\u001B[0m in \u001B[0;36m_do_epoch_train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    196\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_do_epoch_train\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    197\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdl\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdls\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 198\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_with_events\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mall_batches\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'train'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mCancelTrainException\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    199\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    200\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_do_epoch_validate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mds_idx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\fastai\\learner.py\u001B[0m in \u001B[0;36m_with_events\u001B[1;34m(self, f, event_type, ex, final)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    162\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_with_events\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfinal\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnoop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 163\u001B[1;33m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'before_{event_type}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m;\u001B[0m  \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    164\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mex\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'after_cancel_{event_type}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    165\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'after_{event_type}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m;\u001B[0m  \u001B[0mfinal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\fastai\\learner.py\u001B[0m in \u001B[0;36mall_batches\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    167\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mall_batches\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    168\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mn_iter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdl\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 169\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mo\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdl\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mone_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mo\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    170\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    171\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_do_one_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\fastai\\learner.py\u001B[0m in \u001B[0;36mone_batch\u001B[1;34m(self, i, b)\u001B[0m\n\u001B[0;32m    192\u001B[0m         \u001B[0mb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_set_device\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    193\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_split\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 194\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_with_events\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_one_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'batch'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mCancelBatchException\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    195\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    196\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_do_epoch_train\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\fastai\\learner.py\u001B[0m in \u001B[0;36m_with_events\u001B[1;34m(self, f, event_type, ex, final)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    162\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_with_events\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfinal\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnoop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 163\u001B[1;33m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'before_{event_type}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m;\u001B[0m  \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    164\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mex\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'after_cancel_{event_type}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    165\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'after_{event_type}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m;\u001B[0m  \u001B[0mfinal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\adaptnlp\\training\\core.py\u001B[0m in \u001B[0;36m_do_one_batch\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    357\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mxb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mv\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mxb\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m \u001B[1;31m# See if `to_device` fixes this\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    358\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0myb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0myb\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 359\u001B[1;33m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mxb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    360\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;34m'loss'\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    361\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloss_grad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'loss'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1198\u001B[0m         \u001B[0mreturn_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mreturn_dict\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mreturn_dict\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muse_return_dict\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1199\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1200\u001B[1;33m         outputs = self.roberta(\n\u001B[0m\u001B[0;32m   1201\u001B[0m             \u001B[0minput_ids\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1202\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    848\u001B[0m             \u001B[0mpast_key_values_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpast_key_values_length\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    849\u001B[0m         )\n\u001B[1;32m--> 850\u001B[1;33m         encoder_outputs = self.encoder(\n\u001B[0m\u001B[0;32m    851\u001B[0m             \u001B[0membedding_output\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    852\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mextended_attention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    522\u001B[0m                 )\n\u001B[0;32m    523\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 524\u001B[1;33m                 layer_outputs = layer_module(\n\u001B[0m\u001B[0;32m    525\u001B[0m                     \u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    526\u001B[0m                     \u001B[0mattention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    449\u001B[0m             \u001B[0mpresent_key_value\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpresent_key_value\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mcross_attn_present_key_value\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 451\u001B[1;33m         layer_output = apply_chunking_to_forward(\n\u001B[0m\u001B[0;32m    452\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeed_forward_chunk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchunk_size_feed_forward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseq_len_dim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    453\u001B[0m         )\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\modeling_utils.py\u001B[0m in \u001B[0;36mapply_chunking_to_forward\u001B[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[0;32m   2347\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput_chunks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mchunk_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2348\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2349\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mforward_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput_tensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001B[0m in \u001B[0;36mfeed_forward_chunk\u001B[1;34m(self, attention_output)\u001B[0m\n\u001B[0;32m    462\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfeed_forward_chunk\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    463\u001B[0m         \u001B[0mintermediate_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintermediate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 464\u001B[1;33m         \u001B[0mlayer_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mintermediate_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    465\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mlayer_output\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    466\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, input_tensor)\u001B[0m\n\u001B[0;32m    374\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    375\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 376\u001B[1;33m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    377\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    378\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLayerNorm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0minput_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     95\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 96\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     97\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     98\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mlinear\u001B[1;34m(input, weight, bias)\u001B[0m\n\u001B[0;32m   1845\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mhas_torch_function_variadic\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1846\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mhandle_torch_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlinear\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1847\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1848\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1849\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:79] data. DefaultCPUAllocator: not enough memory: you tried to allocate 17825792 bytes."
     ]
    }
   ],
   "source": [
    "from adaptnlp import Strategy\n",
    "\n",
    "tuner.tune(1, lr, strategy=Strategy.OneCycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'allocated_bytes.all.current'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8332/1831514114.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmemory_summary\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mabbreviated\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\memory.py\u001B[0m in \u001B[0;36mmemory_summary\u001B[1;34m(device, abbreviated)\u001B[0m\n\u001B[0;32m    472\u001B[0m             \u001B[0mprefix\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmetric_key\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\".\"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0msubmetric_key\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\".\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    473\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 474\u001B[1;33m             \u001B[0mcurrent\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstats\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mprefix\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"current\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    475\u001B[0m             \u001B[0mpeak\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstats\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mprefix\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"peak\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    476\u001B[0m             \u001B[0mallocated\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstats\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mprefix\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"allocated\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'allocated_bytes.all.current'"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_10144/2224583178.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mgc\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mdel\u001B[0m \u001B[0mvariables\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mgc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'variables' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "del variables\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Strategy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}